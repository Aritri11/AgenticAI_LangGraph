{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import operator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model= ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EvaluationSchema(BaseModel):\n",
    "    #If we simply try to give these in the form of prompt to the model then it might make mistakes i.e. the changes of the model giving different outputs (like for score it might give 'six' instead of '6' as the score). To avoid this we used the concept of structured outputs\n",
    "    feedback: str= Field(description='Detailed feedback for the essay')\n",
    "    score: int = Field(description='Score out of 10', ge=0, le=10)"
   ],
   "id": "633157be632d78e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "structured_model=model.with_structured_output(EvaluationSchema)",
   "id": "c0476fa8070559d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class UPSCState(TypedDict):\n",
    "\n",
    "    essay: str\n",
    "    language_feedback: str\n",
    "    analysis_feedback: str\n",
    "    clarity_feedback: str\n",
    "    overall_feedback: str\n",
    "    individual_scores: Annotated[list[int], operator.add] #as we need to store individual scores(in form of list) from each of language, analysis and clarity node so we use 'operator.add' function that adds all the lists into 1 single list\n",
    "    avg_score: float"
   ],
   "id": "30053f55238d28e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "essay='''Membrane proteins are essential components of biological membranes, facilitating communication and\n",
    "exchange between the cell and its environment. They can be classified into integral (intrinsic) membrane\n",
    "proteins, which are embedded within or span the lipid bilayer, and peripheral (extrinsic) membrane proteins,\n",
    "which are loosely associated with the membrane surface. These proteins are involved in diverse functions such\n",
    "as signaling, transport, and structural support, contributing critically to cellular homeostasis and adaptability.'''"
   ],
   "id": "fb070eee12052923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_language(state: UPSCState):\n",
    "    prompt=f'Evaluate the language quality of the following essay and provide a feedback and assign a score out of 10 \\n {state['essay']} '\n",
    "    output=structured_model.invoke(prompt)\n",
    "    return {'language_feedback': output.feedback, 'individual_scores': [output.score]}\n"
   ],
   "id": "6083ca7391e9222b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_analysis(state: UPSCState):\n",
    "    prompt=f'Evaluate the depth of analysis of the following essay and provide a feedback and assign a score out of 10 \\n {state['essay']} '\n",
    "    output=structured_model.invoke(prompt)\n",
    "    return {'analysis_feedback': output.feedback, 'individual_scores': [output.score]}"
   ],
   "id": "5a637e20bb488ace"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_thought(state: UPSCState):\n",
    "    prompt=f'Evaluate the clarity of thought of the following essay and provide a feedback and assign a score out of 10 \\n {state['essay']} '\n",
    "    output=structured_model.invoke(prompt)\n",
    "    return {'clarity_feedback': output.feedback, 'individual_scores': [output.score]}"
   ],
   "id": "344fc7ca7e9d7fa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def final_evaluation(state: UPSCState):\n",
    "\n",
    "    #summary feedback\n",
    "    prompt= f'Based on the following feedbacks create a summarized feedback \\n language feedback - {state['language_feedback']} \\n depth of analysis feedback - {state['analysis_feedback']} \\n, clarity of thought - {state['clarity_feedback']}'\n",
    "\n",
    "    overall_feedback=model.invoke(prompt).content\n",
    "\n",
    "    #avg calculation\n",
    "    avg_score=sum(state['individual_scores'])/len(state['individual_scores'])\n",
    "\n",
    "    return {'overall_feedback': overall_feedback, 'avg_score': avg_score}"
   ],
   "id": "55638faccaa1bc09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#create the graph\n",
    "graph= StateGraph(UPSCState)\n",
    "\n",
    "#add nodes\n",
    "graph.add_node('evaluate_language',evaluate_language)\n",
    "graph.add_node('evaluate_analysis',evaluate_analysis)\n",
    "graph.add_node('evaluate_thought',evaluate_thought)\n",
    "graph.add_node('final_evaluation',final_evaluation)\n",
    "\n",
    "#add edges\n",
    "graph.add_edge(START, 'evaluate_language')\n",
    "graph.add_edge(START, 'evaluate_analysis')\n",
    "graph.add_edge(START, 'evaluate_thought')\n",
    "#parallel\n",
    "graph.add_edge('evaluate_language', 'final_evaluation')\n",
    "graph.add_edge('evaluate_analysis', 'final_evaluation')\n",
    "graph.add_edge('evaluate_thought', 'final_evaluation')\n",
    "graph.add_edge('final_evaluation',END)\n",
    "\n",
    "#compile\n",
    "workflow=graph.compile()"
   ],
   "id": "2b447877cf947dae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#to see the workflow\n",
    "from IPython.display import Image\n",
    "Image(workflow.get_graph().draw_mermaid_png())"
   ],
   "id": "546151fe69c87b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#execute\n",
    "\n",
    "initial_state={'essay': essay}\n",
    "\n",
    "final_state=workflow.invoke(initial_state)\n",
    "\n",
    "print(final_state)\n"
   ],
   "id": "b74b4cae7911450d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
